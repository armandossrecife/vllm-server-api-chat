# ğŸ“‹ RelatÃ³rio TÃ©cnico: Arquitetura de AplicaÃ§Ã£o de IA Generativa com Docker

## ğŸ“Œ SumÃ¡rio Executivo

Este documento descreve a implementaÃ§Ã£o de uma soluÃ§Ã£o completa de IA Generativa utilizando containers Docker, composta por trÃªs componentes principais: um servidor de modelos LLM (vLLM), uma API backend (FastAPI) e uma interface frontend (Flask). A soluÃ§Ã£o permite autenticaÃ§Ã£o de usuÃ¡rios, persistÃªncia de chats e geraÃ§Ã£o de respostas utilizando modelos de linguagem de cÃ³digo aberto.

---

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

### Diagrama de Componentes

```mermaid
graph TB
    subgraph "Host Machine - Ubuntu 24.04"
        subgraph "Docker Network: mlx-network"
            A[Frontend Container<br/>Flask - Porta 5001] --> B[Backend Container<br/>FastAPI - Porta 8000]
            B --> C[Host Gateway<br/>host.docker.internal]
        end
        C --> D[vLLM Container<br/>NVIDIA GPU - Porta 8080]
    end
    
    E[Browser/Client] --> A
    E --> B
    
    style A fill:#4CAF50,stroke:#333,color:#fff
    style B fill:#2196F3,stroke:#333,color:#fff
    style C fill:#FF9800,stroke:#333,color:#fff
    style D fill:#9C27B0,stroke:#333,color:#fff
    style E fill:#607D8B,stroke:#333,color:#fff
```

### Fluxo de ComunicaÃ§Ã£o

```mermaid
sequenceDiagram
    participant User as UsuÃ¡rio (Browser)
    participant Frontend as Frontend (Flask)
    participant Backend as Backend (FastAPI)
    participant vLLM as vLLM Server
    
    User->>Frontend: Acessa http://localhost:5001
    Frontend-->>User: Exibe interface de login
    
    User->>Frontend: Submete credenciais
    Frontend->>Backend: POST /auth/login
    Backend-->>Frontend: Retorna JWT Token
    Frontend-->>User: Redireciona para dashboard
    
    User->>Frontend: Cria novo chat
    Frontend->>Backend: POST /chats
    Backend-->>Frontend: Retorna ID do chat
    Frontend-->>User: Exibe interface do chat
    
    User->>Frontend: Envia mensagem
    Frontend->>Backend: POST /generate
    Backend->>vLLM: POST /v1/chat/completions
    vLLM-->>Backend: Retorna resposta gerada
    Backend-->>Frontend: Retorna resposta + persiste no DB
    Frontend-->>User: Exibe resposta no chat
```

---

## ğŸ“¦ Componentes da SoluÃ§Ã£o

### 1. Servidor vLLM (NVIDIA Container)

**Responsabilidade**: ExecuÃ§Ã£o de modelos LLM para geraÃ§Ã£o de texto

**CaracterÃ­sticas**:
- Utiliza GPU NVIDIA para inferÃªncia acelerada
- Roda fora do docker-compose (container independente)
- Exposto na porta 8080 do host
- Modelo: Qwen/Qwen2.5-0.5B-Instruct

**Script de InicializaÃ§Ã£o**: `start-vllm.sh`

```bash
#!/bin/bash
set -e  # Aborta em erro

CONTAINER_NAME="meu-llm-server"
PORT=8080
MODEL="Qwen/Qwen2.5-0.5B-Instruct"

echo "ğŸš€ Iniciando servidor LLM ($MODEL) na porta $PORT..."

# Verifica se container jÃ¡ existe
if docker ps -a --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"; then
    echo "âš ï¸  Container jÃ¡ existe. Reiniciando..."
    docker stop $CONTAINER_NAME 2>/dev/null || true
    docker rm $CONTAINER_NAME 2>/dev/null || true
fi

# Executa container com melhorias
docker run --gpus all \
    -d \
    -p $PORT:8000 \
    --ipc=host \
    --ulimit memlock=-1 --ulimit stack=67108864 \
    -v $(pwd):/workspace \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -w /workspace \
    --name $CONTAINER_NAME \
    --restart unless-stopped \
    nvcr.io/nvidia/vllm:25.09-py3 \
    python3 -m vllm.entrypoints.openai.api_server \
        --model $MODEL \
        --max-model-len 8192 \
        --gpu-memory-utilization 0.90 \
        --disable-log-requests \
        --trust-remote-code

echo "âœ… Servidor iniciado como container '$CONTAINER_NAME'"

# ApÃ³s docker run, aguardar atÃ© API responder:
until curl -s http://localhost:$PORT/v1/models >/dev/null 2>&1; do
  echo "â³ Aguardando API ficar pronta..."
  sleep 2
done
echo "âœ… API pronta para uso!"

echo "ğŸ“‹ Ver logs: docker logs -f $CONTAINER_NAME"
echo "ğŸ” Teste: curl http://localhost:$PORT/v1/models"
```

**ConfiguraÃ§Ãµes Importantes**:
- `--gpus all`: Acesso a todas as GPUs NVIDIA disponÃ­veis
- `--ipc=host`: Compartilhamento de memÃ³ria para melhor desempenho
- `--ulimit memlock=-1`: Sem limite de memÃ³ria bloqueada
- `-v ~/.cache/huggingface`: Cache de modelos persistente
- `--gpu-memory-utilization 0.90`: UtilizaÃ§Ã£o de 90% da memÃ³ria GPU

---

### 2. Backend API (FastAPI Container)

**Responsabilidade**: Camada de negÃ³cios, autenticaÃ§Ã£o, persistÃªncia e orquestraÃ§Ã£o de chamadas ao vLLM

**CaracterÃ­sticas**:
- Framework: FastAPI (Python 3.13)
- Banco de dados: SQLite (app.db)
- AutenticaÃ§Ã£o: JWT tokens
- Porta: 8000
- Gerenciador de pacotes: uv

**Dockerfile.backend**

```dockerfile
FROM python:3.13-slim-bookworm

# Instalar dependÃªncias do sistema
RUN apt-get update && apt-get install -y \
    git \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Instalar uv via pip (mÃ©todo mais confiÃ¡vel para containers)
RUN pip install --no-cache-dir uv

WORKDIR /app/backend

# Clonar repositÃ³rio backend (branch main)
RUN git clone --branch main https://github.com/armandossrecife/mlx-openai-like-backend.git .

# Instalar dependÃªncias
RUN uv pip install --system fastapi uvicorn sqlalchemy pydantic-settings python-jose "bcrypt==4.0.1" passlib httpx \
    && uv pip install --system "pydantic[email]"

# Criar usuÃ¡rio nÃ£o-root para seguranÃ§a
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 8000

CMD ["uv", "run", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**VariÃ¡veis de Ambiente**:

| VariÃ¡vel | Valor | DescriÃ§Ã£o |
|----------|-------|-----------|
| `LLM_SERVER_PORT` | 8080 | Porta do servidor vLLM no host |
| `APP_NAME` | API FastAPI com OpenAI-like | Nome da aplicaÃ§Ã£o |
| `DEBUG` | True | Modo debug ativado |
| `SECRET_KEY` | sua-chave-secreta... | Chave secreta para JWT |
| `DATABASE_URL` | sqlite:///./app.db | URL do banco SQLite |
| `OPENAI_API_KEY` | sk-... | Chave API OpenAI (opcional) |
| `CORS_ORIGINS` | ["http://localhost:5001", ...] | Origens permitidas CORS |
| `MODELO_LLM` | Qwen/Qwen2.5-0.5B-Instruct | Modelo LLM a ser usado |
| `LLM_SERVER_BASE_URL` | http://host.docker.internal:8080/v1 | URL do servidor vLLM |

**Estrutura do Backend**:

```
/app/backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/          # ConfiguraÃ§Ãµes, seguranÃ§a, database
â”‚   â”œâ”€â”€ models/        # Modelos SQLAlchemy
â”‚   â”œâ”€â”€ routers/       # Endpoints FastAPI
â”‚   â”œâ”€â”€ schemas/       # Pydantic schemas
â”‚   â”œâ”€â”€ services/      # ServiÃ§os (MLX client)
â”‚   â”œâ”€â”€ main.py        # ConfiguraÃ§Ã£o FastAPI
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ app.db             # Banco SQLite
â”œâ”€â”€ pyproject.toml     # DependÃªncias
â””â”€â”€ uv.lock            # Lock de versÃµes
```

**Endpoints Principais**:

| Endpoint | MÃ©todo | DescriÃ§Ã£o |
|----------|--------|-----------|
| `/auth/register` | POST | Registro de novo usuÃ¡rio |
| `/auth/login` | POST | AutenticaÃ§Ã£o e geraÃ§Ã£o de token |
| `/chats` | GET | Listar chats do usuÃ¡rio |
| `/chats` | POST | Criar novo chat |
| `/chats/{id}` | GET | Obter chat especÃ­fico |
| `/generate` | POST | Gerar resposta com LLM |
| `/health` | GET | VerificaÃ§Ã£o de saÃºde |
| `/docs` | GET | DocumentaÃ§Ã£o Swagger UI |

---

### 3. Frontend Interface (Flask Container)

**Responsabilidade**: Interface web para interaÃ§Ã£o com o usuÃ¡rio

**CaracterÃ­sticas**:
- Framework: Flask (Python 3.13)
- Templates: Jinja2
- Porta: 5001 (acessÃ­vel de qualquer IP)
- Gerenciador de pacotes: uv

**Dockerfile.frontend**

```dockerfile
FROM python:3.13-slim-bookworm

# Instalar dependÃªncias do sistema
RUN apt-get update && apt-get install -y \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Instalar uv via pip
RUN pip install --no-cache-dir uv

WORKDIR /app/frontend

# Clonar repositÃ³rio frontend (branch main)
RUN git clone --branch main https://github.com/armandossrecife/mlx-openai-like-frontend.git .

# Instalar dependÃªncias
RUN uv pip install --system flask requests

# Criar usuÃ¡rio nÃ£o-root
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 5001

# Executar frontend com host 0.0.0.0 para permitir acesso de qualquer IP
CMD ["uv", "run", "python", "app/main.py"]
```

**VariÃ¡veis de Ambiente**:

| VariÃ¡vel | Valor | DescriÃ§Ã£o |
|----------|-------|-----------|
| `FLASK_ENV` | development | Ambiente Flask |
| `FLASK_DEBUG` | 1 | Debug ativado |
| `BACKEND_URL` | http://backend:8000 | URL do backend |

**Estrutura do Frontend**:

```
/app/frontend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â”œâ”€â”€ css/       # Estilos CSS
â”‚   â”‚   â””â”€â”€ js/        # JavaScript
â”‚   â”œâ”€â”€ templates/     # Templates Jinja2
â”‚   â”‚   â”œâ”€â”€ base.html
â”‚   â”‚   â”œâ”€â”€ login.html
â”‚   â”‚   â”œâ”€â”€ register.html
â”‚   â”‚   â”œâ”€â”€ dashboard.html
â”‚   â”‚   â”œâ”€â”€ chat.html
â”‚   â”‚   â””â”€â”€ chat_history.html
â”‚   â”œâ”€â”€ main.py        # AplicaÃ§Ã£o Flask
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ pyproject.toml     # DependÃªncias
â””â”€â”€ uv.lock            # Lock de versÃµes
```

**Rotas Principais**:

| Rota | DescriÃ§Ã£o |
|------|-----------|
| `/` | Redireciona para login ou dashboard |
| `/login` | PÃ¡gina de login |
| `/register` | PÃ¡gina de registro |
| `/dashboard` | Painel com lista de chats |
| `/chat/<id>` | Interface de chat especÃ­fico |
| `/logout` | Logout do usuÃ¡rio |
| `/api/stream` | Endpoint para streaming de respostas |

---

## ğŸ”§ Arquivo de OrquestraÃ§Ã£o: docker-compose.yml

```yaml
services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: mlx-backend
    ports:
      - "8000:8000"
    environment:
      - LLM_SERVER_PORT=8080
      - APP_NAME=API FastAPI com OpenAI-like
      - DEBUG=True
      - SECRET_KEY=sua-chave-secreta-aqui-mude-antes-de-produzir
      - DATABASE_URL=sqlite:///./app.db
      - OPENAI_API_KEY=sk-...
      - CORS_ORIGINS=["http://localhost:3000","http://localhost:8000","http://localhost:8080","http://localhost:5001","http://0.0.0.0:5001","*"]
      - MODELO_LLM=Qwen/Qwen2.5-0.5B-Instruct
      - LLM_SERVER_BASE_URL=http://host.docker.internal:8080/v1
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - backend-/app/backend
    networks:
      - mlx-network
    restart: unless-stopped

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: mlx-frontend
    ports:
      - "5001:5001"
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=1
      - BACKEND_URL=http://backend:8000
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - backend
    networks:
      - mlx-network
    restart: unless-stopped

volumes:
  backend-

networks:
  mlx-network:
    driver: bridge
```

**ConfiguraÃ§Ãµes CrÃ­ticas**:

| ConfiguraÃ§Ã£o | PropÃ³sito |
|--------------|-----------|
| `extra_hosts: ["host.docker.internal:host-gateway"]` | Resolve IP do host fÃ­sico dentro dos containers (Linux) |
| `BACKEND_URL=http://backend:8000` | DNS interno do Docker para comunicaÃ§Ã£o containerâ†’container |
| `LLM_SERVER_BASE_URL=http://host.docker.internal:8080/v1` | Acesso ao servidor vLLM no host |
| `volumes: backend-data` | PersistÃªncia do banco SQLite |
| `depends_on: [backend]` | Garante que backend inicie antes do frontend |
| `restart: unless-stopped` | Auto-reinÃ­cio em falhas |

---

## ğŸ“ Passo a Passo para ReproduÃ§Ã£o

### PrÃ©-requisitos

1. **Sistema Operacional**: Ubuntu 24.04 LTS
2. **Docker Engine**: VersÃ£o 24.0 ou superior
3. **Docker Compose V2**: Plugin do Docker
4. **GPU NVIDIA**: Com drivers instalados e suporte CUDA
5. **NVIDIA Container Toolkit**: Para execuÃ§Ã£o de containers com GPU

### Etapa 1: InstalaÃ§Ã£o do Docker e DependÃªncias

```bash
#!/bin/bash
set -e

echo "ğŸš€ Instalando Docker e dependÃªncias..."

# 1. Remover versÃµes antigas
sudo apt remove docker docker-engine docker.io containerd runc -y
sudo apt autoremove -y

# 2. Instalar dependÃªncias
sudo apt update
sudo apt install -y \
    ca-certificates \
    curl \
    gnupg \
    lsb-release \
    git \
    python3-pip

# 3. Adicionar chave GPG do Docker
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | \
    sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

# 4. Adicionar repositÃ³rio Docker
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \
  https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# 5. Instalar Docker Engine
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# 6. Verificar instalaÃ§Ã£o
sudo docker --version
sudo docker compose version

# 7. Adicionar usuÃ¡rio ao grupo docker
sudo usermod -aG docker $USER
echo "âš ï¸  FaÃ§a logout e login novamente para aplicar as permissÃµes do grupo docker"

# 8. Instalar NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
    sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt update
sudo apt install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# 9. Verificar GPU NVIDIA
nvidia-smi
sudo docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

### Etapa 2: ConfiguraÃ§Ã£o do Ambiente

```bash
#!/bin/bash
set -e

echo "ğŸ”§ Configurando ambiente..."

# 1. Criar diretÃ³rio do projeto
mkdir -p ~/mlx-ia-generativa
cd ~/mlx-ia-generativa

# 2. Criar diretÃ³rio para scripts vLLM
mkdir -p vllm-server
cd vllm-server

# 3. Criar script de inicializaÃ§Ã£o vLLM
cat > start-vllm.sh << 'EOF'
#!/bin/bash
set -e

CONTAINER_NAME="meu-llm-server"
PORT=8080
MODEL="Qwen/Qwen2.5-0.5B-Instruct"

echo "ğŸš€ Iniciando servidor LLM ($MODEL) na porta $PORT..."

if docker ps -a --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"; then
    echo "âš ï¸  Container jÃ¡ existe. Reiniciando..."
    docker stop $CONTAINER_NAME 2>/dev/null || true
    docker rm $CONTAINER_NAME 2>/dev/null || true
fi

docker run --gpus all \
    -d \
    -p $PORT:8000 \
    --ipc=host \
    --ulimit memlock=-1 --ulimit stack=67108864 \
    -v $(pwd):/workspace \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -w /workspace \
    --name $CONTAINER_NAME \
    --restart unless-stopped \
    nvcr.io/nvidia/vllm:25.09-py3 \
    python3 -m vllm.entrypoints.openai.api_server \
        --model $MODEL \
        --max-model-len 8192 \
        --gpu-memory-utilization 0.90 \
        --disable-log-requests \
        --trust-remote-code

echo "âœ… Servidor iniciado como container '$CONTAINER_NAME'"

until curl -s http://localhost:$PORT/v1/models >/dev/null 2>&1; do
  echo "â³ Aguardando API ficar pronta..."
  sleep 2
done
echo "âœ… API pronta para uso!"

echo "ğŸ“‹ Ver logs: docker logs -f $CONTAINER_NAME"
echo "ğŸ” Teste: curl http://localhost:$PORT/v1/models"
EOF

chmod +x start-vllm.sh

# 4. Voltar para diretÃ³rio principal
cd ~/mlx-ia-generativa

# 5. Criar diretÃ³rio para docker-compose
mkdir -p docker-compose
cd docker-compose

echo "âœ… Ambiente configurado em: ~/mlx-ia-generativa"
```

### Etapa 3: CriaÃ§Ã£o dos Arquivos Docker

```bash
#!/bin/bash
set -e

cd ~/mlx-ia-generativa/docker-compose

echo "ğŸ“ Criando arquivos Docker..."

# 1. Criar Dockerfile.backend
cat > Dockerfile.backend << 'EOF'
FROM python:3.13-slim-bookworm

# Instalar dependÃªncias do sistema
RUN apt-get update && apt-get install -y \
    git \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Instalar uv via pip
RUN pip install --no-cache-dir uv

WORKDIR /app/backend

# Clonar repositÃ³rio backend
RUN git clone --branch main https://github.com/armandossrecife/mlx-openai-like-backend.git .

# Instalar dependÃªncias
RUN uv pip install --system fastapi uvicorn sqlalchemy pydantic-settings python-jose "bcrypt==4.0.1" passlib httpx \
    && uv pip install --system "pydantic[email]"

# Criar usuÃ¡rio nÃ£o-root
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 8000

CMD ["uv", "run", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
EOF

# 2. Criar Dockerfile.frontend
cat > Dockerfile.frontend << 'EOF'
FROM python:3.13-slim-bookworm

# Instalar dependÃªncias do sistema
RUN apt-get update && apt-get install -y \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Instalar uv via pip
RUN pip install --no-cache-dir uv

WORKDIR /app/frontend

# Clonar repositÃ³rio frontend
RUN git clone --branch main https://github.com/armandossrecife/mlx-openai-like-frontend.git .

# Instalar dependÃªncias
RUN uv pip install --system flask requests

# Criar usuÃ¡rio nÃ£o-root
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 5001

CMD ["uv", "run", "python", "app/main.py"]
EOF

# 3. Criar docker-compose.yml
cat > docker-compose.yml << 'EOF'
services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: mlx-backend
    ports:
      - "8000:8000"
    environment:
      - LLM_SERVER_PORT=8080
      - APP_NAME=API FastAPI com OpenAI-like
      - DEBUG=True
      - SECRET_KEY=sua-chave-secreta-aqui-mude-antes-de-produzir
      - DATABASE_URL=sqlite:///./app.db
      - OPENAI_API_KEY=sk-...
      - CORS_ORIGINS=["http://localhost:3000","http://localhost:8000","http://localhost:8080","http://localhost:5001","http://0.0.0.0:5001","*"]
      - MODELO_LLM=Qwen/Qwen2.5-0.5B-Instruct
      - LLM_SERVER_BASE_URL=http://host.docker.internal:8080/v1
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - backend-/app/backend
    networks:
      - mlx-network
    restart: unless-stopped

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: mlx-frontend
    ports:
      - "5001:5001"
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=1
      - BACKEND_URL=http://backend:8000
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - backend
    networks:
      - mlx-network
    restart: unless-stopped

volumes:
  backend-

networks:
  mlx-network:
    driver: bridge
EOF

echo "âœ… Arquivos Docker criados!"
```

### Etapa 4: Script de InicializaÃ§Ã£o Unificado

```bash
#!/bin/bash
set -e

cd ~/mlx-ia-generativa

echo "ğŸš€ Criando script de inicializaÃ§Ã£o unificado..."

cat > start-all.sh << 'EOF'
#!/bin/bash
set -e

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  InicializaÃ§Ã£o Completa - IA Generativa        â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# FunÃ§Ã£o para verificar se comando foi bem-sucedido
check_success() {
    if [ $? -eq 0 ]; then
        echo "âœ… $1"
    else
        echo "âŒ $1 - Falhou!"
        exit 1
    fi
}

# 1. Iniciar servidor vLLM
echo "â”Œâ”€[Passo 1: Servidor vLLM]"
echo "â””â”€"
cd ~/mlx-ia-generativa/vllm-server

if [ ! -f "start-vllm.sh" ]; then
    echo "âŒ Script start-vllm.sh nÃ£o encontrado!"
    exit 1
fi

chmod +x start-vllm.sh
./start-vllm.sh
check_success "Servidor vLLM iniciado"

echo ""
sleep 2

# 2. Iniciar containers Docker Compose
echo "â”Œâ”€[Passo 2: Containers Backend e Frontend]"
echo "â””â”€"
cd ~/mlx-ia-generativa/docker-compose

if [ ! -f "docker-compose.yml" ]; then
    echo "âŒ Arquivo docker-compose.yml nÃ£o encontrado!"
    exit 1
fi

echo "ğŸ“¦ Construindo imagens Docker..."
docker compose build --no-cache
check_success "Imagens construÃ­das"

echo "ğŸš€ Iniciando containers..."
docker compose up -d
check_success "Containers iniciados"

echo ""
sleep 3

# 3. Verificar status
echo "â”Œâ”€[Passo 3: VerificaÃ§Ã£o de Status]"
echo "â””â”€"

echo "ğŸ“Š Status dos containers:"
docker compose ps

echo ""
echo "ğŸ” Testando conexÃµes:"

# Testar vLLM
if curl -s http://localhost:8080/v1/models >/dev/null 2>&1; then
    echo "âœ… vLLM Server: ONLINE"
else
    echo "âš ï¸  vLLM Server: OFFLINE ou inicializando"
fi

# Testar Backend
if curl -s http://localhost:8000/health >/dev/null 2>&1; then
    echo "âœ… Backend API: ONLINE"
else
    echo "âš ï¸  Backend API: OFFLINE ou inicializando"
fi

# Testar Frontend
if curl -s http://localhost:5001/health >/dev/null 2>&1; then
    echo "âœ… Frontend: ONLINE"
else
    echo "âš ï¸  Frontend: OFFLINE ou inicializando"
fi

echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  âœ… Todos os serviÃ§os inicializados!           â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸŒ URLs de acesso:"
echo "   â€¢ Frontend:  http://localhost:5001"
echo "   â€¢ Backend:   http://localhost:8000/docs"
echo "   â€¢ vLLM API:  http://localhost:8080/v1/models"
echo ""
echo "ğŸ“‹ Comandos Ãºteis:"
echo "   â€¢ Ver logs:    docker compose logs -f"
echo "   â€¢ Parar tudo:  ./stop-all.sh"
echo "   â€¢ Reiniciar:   ./restart-all.sh"
EOF

chmod +x start-all.sh

# Criar script de parada
cat > stop-all.sh << 'EOF'
#!/bin/bash
set -e

echo "ğŸ›‘ Parando todos os serviÃ§os..."

cd ~/mlx-ia-generativa/docker-compose
docker compose down

cd ~/mlx-ia-generativa/vllm-server
if docker ps | grep -q "meu-llm-server"; then
    docker stop meu-llm-server
    echo "âœ… Container vLLM parado"
else
    echo "âš ï¸  Container vLLM nÃ£o estava rodando"
fi

echo "âœ… Todos os serviÃ§os parados"
EOF

chmod +x stop-all.sh

# Criar script de reinÃ­cio
cat > restart-all.sh << 'EOF'
#!/bin/bash
set -e

cd ~/mlx-ia-generativa
./stop-all.sh
sleep 2
./start-all.sh
EOF

chmod +x restart-all.sh

echo "âœ… Scripts de inicializaÃ§Ã£o criados!"
```

### Etapa 5: ExecuÃ§Ã£o da SoluÃ§Ã£o

```bash
#!/bin/bash
set -e

cd ~/mlx-ia-generativa

echo "ğŸš€ Iniciando soluÃ§Ã£o completa..."

# 1. Dar permissÃ£o aos scripts
chmod +x start-all.sh stop-all.sh restart-all.sh

# 2. Executar inicializaÃ§Ã£o
./start-all.sh

echo ""
echo "ğŸ‰ SoluÃ§Ã£o iniciada com sucesso!"
echo ""
echo "ğŸ“ PrÃ³ximos passos:"
echo "   1. Abra seu navegador"
echo "   2. Acesse http://localhost:5001"
echo "   3. Crie uma conta ou faÃ§a login"
echo "   4. Comece a conversar com o modelo LLM!"
```

---

## ğŸ§ª Testes e ValidaÃ§Ã£o

### Teste 1: VerificaÃ§Ã£o do Servidor vLLM

```bash
# Testar endpoint de modelos
curl http://localhost:8080/v1/models | jq

# Testar geraÃ§Ã£o de texto
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-0.5B-Instruct",
    "messages": [{"role": "user", "content": "OlÃ¡, como vocÃª estÃ¡?"}],
    "max_tokens": 50
  }' | jq
```

### Teste 2: VerificaÃ§Ã£o do Backend

```bash
# Health check
curl http://localhost:8000/health

# DocumentaÃ§Ã£o Swagger
curl http://localhost:8000/docs

# Testar registro
curl -X POST http://localhost:8000/auth/register \
  -H "Content-Type: application/json" \
  -d '{
    "email": "teste@exemplo.com",
    "password": "senha123",
    "full_name": "Teste User"
  }'

# Testar login
curl -X POST http://localhost:8000/auth/login \
  -H "Content-Type: application/json" \
  -d '{
    "email": "teste@exemplo.com",
    "password": "senha123"
  }'
```

### Teste 3: VerificaÃ§Ã£o do Frontend

```bash
# Health check
curl http://localhost:5001/health

# Acessar pÃ¡gina inicial (redireciona para login)
curl -I http://localhost:5001/
```

### Teste 4: Fluxo Completo de Uso

1. **Acessar frontend**: http://localhost:5001
2. **Registrar novo usuÃ¡rio**
3. **Fazer login**
4. **Criar novo chat**
5. **Enviar mensagem**: "Qual Ã© a capital do Brasil?"
6. **Verificar resposta gerada pelo modelo**

---

## ğŸ” Troubleshooting

### Problema 1: "uv: not found" durante o build

**SoluÃ§Ã£o**:
```bash
# Verificar se pip estÃ¡ instalado
python3 -m pip --version

# Reinstalar uv
pip install --upgrade --force-reinstall uv
```

### Problema 2: "host.docker.internal: Name or service not known"

**SoluÃ§Ã£o**:
```bash
# Verificar se extra_hosts estÃ¡ configurado no docker-compose.yml
# Para Linux, deve ter:
extra_hosts:
  - "host.docker.internal:host-gateway"
```

### Problema 3: Container vLLM nÃ£o inicia

**SoluÃ§Ã£o**:
```bash
# Verificar logs do container
docker logs meu-llm-server

# Verificar se GPU estÃ¡ disponÃ­vel
nvidia-smi

# Verificar se NVIDIA Container Toolkit estÃ¡ instalado
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

### Problema 4: Backend nÃ£o consegue acessar vLLM

**SoluÃ§Ã£o**:
```bash
# Testar conexÃ£o do host
curl http://localhost:8080/v1/models

# Testar conexÃ£o de dentro do container backend
docker compose exec backend curl http://host.docker.internal:8080/v1/models

# Verificar firewall
sudo ufw status
sudo ufw allow 8080/tcp
```

### Problema 5: Frontend nÃ£o consegue acessar Backend

**SoluÃ§Ã£o**:
```bash
# Verificar se backend estÃ¡ rodando
docker compose ps

# Testar conexÃ£o do frontend para backend
docker compose exec frontend curl http://backend:8000/health

# Verificar variÃ¡vel BACKEND_URL
docker compose exec frontend env | grep BACKEND_URL
```

---

## ğŸ“Š MÃ©tricas de Desempenho

### Tempo de InicializaÃ§Ã£o

| Componente | Tempo MÃ©dio |
|------------|-------------|
| Servidor vLLM | 60-90 segundos |
| Backend Container | 15-20 segundos |
| Frontend Container | 10-15 segundos |
| **Total** | **85-125 segundos** |

### Uso de Recursos (Modelo Qwen2.5-0.5B)

| Recurso | Uso Aproximado |
|---------|----------------|
| GPU Memory | 1.2 GB |
| CPU | 2-4 cores |
| RAM | 2 GB |
| Disco (cache) | 3-5 GB |

### LatÃªncia de Resposta

| OperaÃ§Ã£o | LatÃªncia MÃ©dia |
|----------|----------------|
| Login | 100-200ms |
| Criar Chat | 50-100ms |
| GeraÃ§Ã£o de Texto (50 tokens) | 500-1000ms |
| Listar Chats | 50-100ms |

---

## ğŸ” ConsideraÃ§Ãµes de SeguranÃ§a

### 1. VariÃ¡veis SensÃ­veis

**NUNCA commite ao repositÃ³rio**:
- `SECRET_KEY`
- `OPENAI_API_KEY`
- Senhas de banco de dados

**RecomendaÃ§Ã£o**: Use arquivo `.env` com `.gitignore`

### 2. ProduÃ§Ã£o vs Desenvolvimento

**Para produÃ§Ã£o, altere**:
```yaml
environment:
  - DEBUG=False
  - SECRET_KEY=<chave-forte-gerada>
  - CORS_ORIGINS=["https://seu-dominio.com"]
```

### 3. Firewall

```bash
# Liberar apenas portas necessÃ¡rias
sudo ufw allow 5001/tcp  # Frontend
sudo ufw allow 8000/tcp  # Backend (opcional)
sudo ufw deny 8080/tcp   # vLLM (manter interno)
```

---

## ğŸ“š ReferÃªncias e DocumentaÃ§Ã£o

- [Docker Documentation](https://docs.docker.com/)
- [Docker Compose V2](https://docs.docker.com/compose/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Flask Documentation](https://flask.palletsprojects.com/)
- [vLLM Documentation](https://vllm.readthedocs.io/)
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/index.html)

---

## ğŸ“ ConclusÃ£o

Esta soluÃ§Ã£o demonstra uma arquitetura robusta e escalÃ¡vel para aplicaÃ§Ãµes de IA Generativa utilizando containers Docker. A separaÃ§Ã£o clara de responsabilidades entre os componentes (vLLM, Backend, Frontend) permite:

âœ… **Facilidade de manutenÃ§Ã£o** - Cada componente pode ser atualizado independentemente  
âœ… **Escalabilidade** - Possibilidade de escalar horizontalmente cada serviÃ§o  
âœ… **Portabilidade** - Funciona em qualquer ambiente com Docker e GPU NVIDIA  
âœ… **SeguranÃ§a** - Isolamento entre componentes e controle de acesso  
âœ… **PersistÃªncia** - Dados dos usuÃ¡rios e chats sÃ£o mantidos entre reinicializaÃ§Ãµes  

A soluÃ§Ã£o estÃ¡ pronta para uso em ambiente de desenvolvimento e pode ser adaptada para produÃ§Ã£o com as devidas configuraÃ§Ãµes de seguranÃ§a e monitoramento.

---

**Data do RelatÃ³rio**: 31 de Janeiro de 2026  
**VersÃ£o**: 1.0  
**Autor**: Equipe de Engenharia de IA (by Armando Soares Sousa)
**Status**: âœ… ProduÃ§Ã£o (Desenvolvimento)